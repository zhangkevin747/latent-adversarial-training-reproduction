{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48ec2847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0707c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: INITIALIZATION ---\n",
    "# Task: Load a small model for CPU/local GPU testing.\n",
    "# Recommended: \"google/gemma-2b\" or \"EleutherAI/pythia-70m\"\n",
    "\n",
    "model_id = \"EleutherAI/pythia-70m\" # Lightweight for local testing\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# TODO: Define your target layers. \n",
    "# The paper uses [8, 16, 24, 30] for Llama-2-7B. \n",
    "# For Pythia-70m (6 layers), use layer.\n",
    "target_layers = [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64e549ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x19e153ec3d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- STEP 2: THE LATENT HOOK ---\n",
    "# Task: Create a hook that adds a perturbation to the layer's output.\n",
    "\n",
    "class LATHook:\n",
    "    def __init__(self):\n",
    "        # delta will be the adversarial perturbation tensor\n",
    "        self.delta = None \n",
    "\n",
    "    def __call__(self, module, input, output):\n",
    "        # TODO: Implement the injection logic.\n",
    "        # Requirement: output_new = output + delta[cite: 1211].\n",
    "        tensor = output[0] + self.delta\n",
    "        output_new = (tensor,) + output[1:]\n",
    "        # Documentation Tip: 'output' is often a tuple for some models (hidden_states, ...).\n",
    "        # Ensure you handle the tensor correctly.\n",
    "        return output_new\n",
    "\n",
    "# --- STEP 3: REGISTER HOOKS ---\n",
    "# Task: Attach instances of LATHook to the model's residual stream.\n",
    "# Documentation Hunt: Look up `model.named_modules()` to find the correct layer paths.\n",
    "\n",
    "hooks = {idx: LATHook() for idx in target_layers}\n",
    "# TODO: Use `register_forward_hook` to attach them.\n",
    "model.gpt_neox.layers[3].register_forward_hook(hooks[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2b122b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.990674018859863\n",
      "7.920614242553711\n",
      "6.929909706115723\n",
      "6.012196063995361\n",
      "5.149251937866211\n",
      "4.3205084800720215\n",
      "3.5313620567321777\n",
      "2.785336971282959\n",
      "2.0899229049682617\n",
      "1.4594265222549438\n",
      "0.9444214701652527\n",
      "0.8326348662376404\n",
      "0.7276362180709839\n",
      "0.653099536895752\n",
      "0.5828970670700073\n",
      "0.5311400294303894\n",
      "0.48197075724601746\n",
      "0.44995179772377014\n",
      "0.4190617799758911\n",
      "0.3955107033252716\n",
      "Final Prediction:  Berlin\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 4: PGD PROTOTYPE ---\n",
    "# Task: Optimize a single 'delta' tensor to minimize loss on a target.\n",
    "\n",
    "def smoke_test_pgd(prompt_text, target_text):\n",
    "    # 1. Tokenize inputs\n",
    "    tokens = tokenizer(prompt_text, return_tensors = \"pt\")\n",
    "    # 2. Initialize delta: torch.zeros(...) with requires_grad=True\n",
    "    delta = torch.zeros([1, len(tokens['input_ids'][0]), model.config.hidden_size], device = model.device, requires_grad= True)\n",
    "    hooks[4].delta = delta\n",
    "    # 3. Optimization Loop (5 steps):\n",
    "    lr = 0.01\n",
    "    epsilon = 0.1\n",
    "    for i in range(20):\n",
    "        #    a. Clear gradients\n",
    "        if delta.grad is not None:\n",
    "            delta.grad.zero_()\n",
    "        #    b. Forward pass (hooks will inject delta)\n",
    "        outputs = model(**tokens)\n",
    "        logits = outputs.logits #get the logits \n",
    "        last_token_logits = logits[:,  -1, :] #we care about the last tokens\n",
    "\n",
    "        target_ids = tokenizer.encode( \" \" + target_text.strip(), add_special_tokens=False, return_tensors = \"pt\")\n",
    "        target_id = target_ids[:, 0] # Take only the first token ID\n",
    "\n",
    "        #    c. Calculate CrossEntropyLoss against target_text\n",
    "        loss = F.cross_entropy(last_token_logits, target_id)\n",
    "        #    d. Backward pass\n",
    "        loss.backward()\n",
    "        #    e. Update delta: delta = delta - lr * delta.grad.sign()\n",
    "        with torch.no_grad():\n",
    "            delta.data -= lr * delta.grad.sign()\n",
    "        #    f. Project: Ensure delta norm <= epsilon[cite: 1219, 2026].\n",
    "            delta.data = torch.clamp(delta.data, -epsilon, epsilon)\n",
    "        print(loss.item())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #We make a final forward pass with the optimized delta \n",
    "        final_outputs = model(**tokens)\n",
    "        final_logits = final_outputs.logits[:, -1, :]\n",
    "\n",
    "        #Top predicted word\n",
    "        predicted_id = torch.argmax(final_logits, dim=-1)\n",
    "        predicted_word = tokenizer.decode(predicted_id)\n",
    "\n",
    "        print(f\"Final Prediction: {predicted_word}\")\n",
    "\n",
    "# Run a test\n",
    "smoke_test_pgd(\"The capital of France is\", \"Berlin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opendataval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
